{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44502e5-0cb2-4529-a38d-b903801ab26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab827189-1e17-4827-828e-b6c37427317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "675e0c07-7cbc-4015-97a6-cd91338f5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1/255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de20b6cd-158c-4c8d-a9ce-454e5e1e866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8048 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64,64),\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e01910f-67ad-4c86-bfbf-8cb94dd57088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c136bd18-3949-4e67-8476-bcd6bbaeaa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64,64),\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76134a5c-8c0d-4033-a9de-c2c359404647",
   "metadata": {},
   "source": [
    "## Modelling - Convolutional Neural Network\n",
    "\n",
    "**Initialising the CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2eb6248-c8db-4633-9fa5-1adaae469c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ffdc5c-34fc-4af6-88db-8e4fc99309f3",
   "metadata": {},
   "source": [
    "**Step-1: Convolutional**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cad0c02-6862-414c-892e-bc82e2cc751c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D\n",
    "classifier.add(Conv2D(input_shape=[64,64,3],\n",
    "                      filters=32, kernel_size=3,activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e601ccc-d84a-433a-a6b3-573efce2a6e2",
   "metadata": {},
   "source": [
    "**Step-2: Max Pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5926af21-93dd-4b89-b662-9e213619eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "classifier.add(MaxPooling2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a075f9-312e-475d-af13-45c268a8137b",
   "metadata": {},
   "source": [
    "**Step-3: Flattening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d5404d-2138-4b56-ab3c-f686ab4aba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2747661-db62-4377-b924-6fc49d032d2d",
   "metadata": {},
   "source": [
    "**Step-4: Full Connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d362b2c-422c-407b-96be-245755187079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "# hidden layer with 128 neurons\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "\n",
    "# output layer with 1 neuron\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d17f8-fa8d-46db-96cd-c32e9d5fff02",
   "metadata": {},
   "source": [
    "**Training the CNN Model with train data & Testing the model with test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02ad4b62-e88b-48d7-a07f-e63a25526172",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam',\n",
    "                   loss = 'binary_crossentropy',\n",
    "                   metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6edf89ec-ee7e-484d-b340-6727dcc2b6d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.5592 - loss: 0.8834"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 134ms/step - accuracy: 0.5594 - loss: 0.8827 - val_accuracy: 0.6850 - val_loss: 0.6083\n",
      "Epoch 2/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 134ms/step - accuracy: 0.6725 - loss: 0.6018 - val_accuracy: 0.6980 - val_loss: 0.5790\n",
      "Epoch 3/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - accuracy: 0.7196 - loss: 0.5441 - val_accuracy: 0.6655 - val_loss: 0.6224\n",
      "Epoch 4/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - accuracy: 0.7229 - loss: 0.5352 - val_accuracy: 0.6550 - val_loss: 0.6775\n",
      "Epoch 5/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - accuracy: 0.7324 - loss: 0.5183 - val_accuracy: 0.7375 - val_loss: 0.5594\n",
      "Epoch 6/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 134ms/step - accuracy: 0.7662 - loss: 0.4760 - val_accuracy: 0.7455 - val_loss: 0.5482\n",
      "Epoch 7/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 133ms/step - accuracy: 0.7801 - loss: 0.4583 - val_accuracy: 0.7460 - val_loss: 0.5457\n",
      "Epoch 8/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 136ms/step - accuracy: 0.7852 - loss: 0.4464 - val_accuracy: 0.7635 - val_loss: 0.5300\n",
      "Epoch 9/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 134ms/step - accuracy: 0.8073 - loss: 0.4233 - val_accuracy: 0.7390 - val_loss: 0.5778\n",
      "Epoch 10/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 136ms/step - accuracy: 0.8072 - loss: 0.4057 - val_accuracy: 0.7605 - val_loss: 0.5726\n",
      "Epoch 11/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 133ms/step - accuracy: 0.8299 - loss: 0.3818 - val_accuracy: 0.7675 - val_loss: 0.5842\n",
      "Epoch 12/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 135ms/step - accuracy: 0.8352 - loss: 0.3614 - val_accuracy: 0.7630 - val_loss: 0.5458\n",
      "Epoch 13/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 133ms/step - accuracy: 0.8552 - loss: 0.3356 - val_accuracy: 0.7595 - val_loss: 0.5640\n",
      "Epoch 14/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 134ms/step - accuracy: 0.8643 - loss: 0.3131 - val_accuracy: 0.7620 - val_loss: 0.5799\n",
      "Epoch 15/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 134ms/step - accuracy: 0.8771 - loss: 0.2929 - val_accuracy: 0.7640 - val_loss: 0.6427\n",
      "Epoch 16/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 136ms/step - accuracy: 0.8752 - loss: 0.2882 - val_accuracy: 0.7700 - val_loss: 0.6288\n",
      "Epoch 17/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 137ms/step - accuracy: 0.9003 - loss: 0.2500 - val_accuracy: 0.7700 - val_loss: 0.6854\n",
      "Epoch 18/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 139ms/step - accuracy: 0.8993 - loss: 0.2414 - val_accuracy: 0.7440 - val_loss: 0.7181\n",
      "Epoch 19/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 139ms/step - accuracy: 0.9155 - loss: 0.2124 - val_accuracy: 0.7665 - val_loss: 0.7308\n",
      "Epoch 20/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 134ms/step - accuracy: 0.9168 - loss: 0.2021 - val_accuracy: 0.7680 - val_loss: 0.7731\n",
      "Epoch 21/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 133ms/step - accuracy: 0.9286 - loss: 0.1882 - val_accuracy: 0.7620 - val_loss: 0.7535\n",
      "Epoch 22/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - accuracy: 0.9295 - loss: 0.1682 - val_accuracy: 0.7435 - val_loss: 0.8235\n",
      "Epoch 23/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 136ms/step - accuracy: 0.9256 - loss: 0.1837 - val_accuracy: 0.7355 - val_loss: 0.9831\n",
      "Epoch 24/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 136ms/step - accuracy: 0.9429 - loss: 0.1504 - val_accuracy: 0.7515 - val_loss: 0.9294\n",
      "Epoch 25/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 145ms/step - accuracy: 0.9467 - loss: 0.1381 - val_accuracy: 0.7660 - val_loss: 0.8732\n",
      "Training took 852.3103098869324 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "classifier.fit(x = training_set, validation_data = test_set, epochs = 25)\n",
    "end_time = time.time()\n",
    "print(f\"Training took {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e4de2-48de-4b33-b142-0874bd7ccf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in gpu it takes 552.2584829330444 seconds accuracy 95\n",
    "# in cpu it takes 852.3103098869324 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f654475-3bbb-4a34-a22c-c5eba72bd2e4",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "- **Make a single prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "991bee63-cfcf-4ce6-91d9-9a0a8570b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc0892d3-739e-4cd0-955d-d5794716ff3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Dog\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "test_image = Image.open('dataset/single_prediction/cat_or_dog_2.jpg')\n",
    "\n",
    "# Data Preprocessing\n",
    "test_image = test_image.resize((64,64))\n",
    "test_image = np.array(test_image)\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "\n",
    "# prediction\n",
    "result = classifier.predict(test_image)\n",
    "\n",
    "# Evaluation\n",
    "if result[0][0] == 1:\n",
    "    print('Dog')\n",
    "else:\n",
    "    print('Cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "652d299c-aed2-49d9-aeef-aaaac714bf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1998d4-e0e6-49df-a9a9-f693632ecfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13c455-e8b6-449f-b30f-ae3c03dfde3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f4049b-01e3-4376-bd28-f8af7ac53467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
